# Migration de rl-games vers RSL-RL pour Isaac Lab

## Vue d'ensemble

Ce document d√©taille la migration compl√®te du syst√®me d'entra√Ænement de **rl-games** vers **RSL-RL** (Robot Systems Lab - Reinforcement Learning), le framework officiel d'Isaac Lab.

### Objectif
Migrer l'int√©gralit√© du code d'entra√Ænement pour utiliser RSL-RL au lieu de rl-games, tout en conservant les m√™mes fonctionnalit√©s et r√©sultats.

---

## 1. Architecture actuelle (rl-games)

### 1.1 Composants principaux

#### Fichiers actuels √† migrer/remplacer:
- `awd_isaaclab/scripts/run_isaaclab.py` - Script principal d'entra√Ænement
- `old_awd/learning/common_agent.py` - Agent de base (h√©rite de `a2c_continuous.A2CAgent`)
- `old_awd/learning/awd_agent.py` - Agent AWD (style imitatif)
- `old_awd/learning/amp_agent.py` - Agent AMP (Adversarial Motion Priors)
- `old_awd/learning/hrl_agent.py` - Agent HRL (Hierarchical RL)
- `old_awd/learning/*_players.py` - Players pour l'inf√©rence
- `old_awd/learning/*_models.py` - Mod√®les de r√©seau
- `old_awd/learning/*_network_builder.py` - Constructeurs de r√©seau

#### Configuration actuelle:
- Fichiers YAML s√©par√©s pour environnement et entra√Ænement
- Exemple: `old_awd/data/cfg/go_bdx/duckling_command.yaml` (env)
- Exemple: `old_awd/data/cfg/go_bdx/train/awd_duckling.yaml` (training)

### 1.2 Flux d'ex√©cution actuel

```
run_isaaclab.py
  ‚îú‚îÄ> AppLauncher (Isaac Sim)
  ‚îú‚îÄ> gymnasium.make() -> Cr√©e env IsaacLab
  ‚îú‚îÄ> RLGPUEnv wrapper -> Convertit Gymnasium -> rl-games API
  ‚îú‚îÄ> rl_games.Runner
  ‚îÇ   ‚îú‚îÄ> Enregistre agents custom (awd, amp, hrl)
  ‚îÇ   ‚îú‚îÄ> Enregistre players custom
  ‚îÇ   ‚îú‚îÄ> Enregistre models custom
  ‚îÇ   ‚îî‚îÄ> Enregistre network builders custom
  ‚îî‚îÄ> runner.run() -> Entra√Ænement
```

### 1.3 √âl√©ments cl√©s √† pr√©server

#### Agents personnalis√©s:
1. **CommonAgent** (PPO de base)
   - Normalisation input/value
   - Bounds loss
   - Central value network (optionnel)

2. **AWDAgent** (AMP with Diversity - Style imitatif)
   - Discriminateur pour imitation
   - Encodeur pour diversit√©
   - Replay buffer AMP
   - Latent space pour styles

3. **AMPAgent** (Adversarial Motion Priors)
   - Discriminateur uniquement
   - Replay buffer AMP
   - Motion priors

4. **HRLAgent** (Hierarchical RL)
   - Latent skills
   - High-level policy
   - Low-level policy

#### Hyperparam√®tres importants:
- PPO: `horizon_length=32`, `minibatch_size=16384`, `mini_epochs=6`
- Learning rate: `2e-5` (constant)
- AMP: `disc_coef=5`, `disc_reward_scale=2`
- AWD: `enc_coef=5`, `disc_reward_w=0.5`, `enc_reward_w=0.5`

---

## 2. Architecture cible (RSL-RL)

### 2.1 Structure RSL-RL

RSL-RL est structur√© autour de:
- `rsl_rl.runners.OnPolicyRunner` - G√®re la boucle d'entra√Ænement
- `rsl_rl.algorithms.PPO` - Algorithme PPO
- `rsl_rl.modules.ActorCritic` - R√©seau acteur-critique
- `rsl_rl.env.VecEnv` - Interface environnement vectoris√©

### 2.2 Int√©gration Isaac Lab

Isaac Lab fournit:
- `isaaclab.utils.wrappers.rsl_rl.RslRlVecEnvWrapper` - Wrapper pour DirectRLEnv
- `isaaclab.utils.wrappers.rsl_rl.RslRlOnPolicyRunnerCfg` - Configuration runner
- Exemples dans `IsaacLab/source/standalone/workflows/rsl_rl/`

---

## 3. Plan de migration d√©taill√©

### ‚úÖ Phase 0: Pr√©paration (COMPL√âT√â)
- [x] Analyse du code actuel rl-games
- [x] Identification des composants √† migrer
- [x] Cr√©ation de ce document de suivi

### üîÑ Phase 1: Configuration de base

#### 1.1 Installer RSL-RL
- [ ] V√©rifier si RSL-RL est d√©j√† install√© avec Isaac Lab
- [ ] Si non: `pip install rsl-rl` ou utiliser la version bundled

#### 1.2 Cr√©er structure de configuration RSL-RL
- [ ] Cr√©er `awd_isaaclab/configs/train/` pour configs d'entra√Ænement
- [ ] Convertir configs YAML rl-games en dataclasses Python RSL-RL
- [ ] Cr√©er `DucklingCommandPPORunnerCfg` (bas√© sur RslRlOnPolicyRunnerCfg)
- [ ] Cr√©er `DucklingHeadingPPORunnerCfg`
- [ ] Cr√©er configs pour autres t√¢ches (Perturb, AMP, etc.)

**Fichiers √† cr√©er:**
- `awd_isaaclab/configs/train/duckling_command_ppo_cfg.py`
- `awd_isaaclab/configs/train/duckling_heading_ppo_cfg.py`
- `awd_isaaclab/configs/train/duckling_amp_cfg.py` (pour AMP)
- `awd_isaaclab/configs/train/duckling_awd_cfg.py` (pour AWD)

### üîÑ Phase 2: Script d'entra√Ænement de base (PPO simple)

#### 2.1 Cr√©er nouveau script train
- [ ] Cr√©er `awd_isaaclab/scripts/train_rsl_rl.py`
- [ ] Impl√©menter AppLauncher pour Isaac Sim
- [ ] Cr√©er environnement avec DirectRLEnv
- [ ] Wrapper avec `RslRlVecEnvWrapper`
- [ ] Cr√©er `OnPolicyRunner` de RSL-RL
- [ ] Impl√©menter boucle d'entra√Ænement

**Code de r√©f√©rence:**
```python
# Exemple structure
from isaaclab.app import AppLauncher
from isaaclab.utils.wrappers.rsl_rl import RslRlVecEnvWrapper, RslRlOnPolicyRunnerCfg
from rsl_rl.runners import OnPolicyRunner

# 1. Launch Isaac Sim
launcher = AppLauncher(...)
simulation_app = launcher.app

# 2. Create environment
env = gymnasium.make(...)

# 3. Wrap for RSL-RL
env = RslRlVecEnvWrapper(env)

# 4. Create runner
runner_cfg = RslRlOnPolicyRunnerCfg(...)
runner = OnPolicyRunner(env, runner_cfg)

# 5. Train
runner.learn(num_learning_iterations=10000)
```

#### 2.2 Tester avec DucklingCommand
- [ ] Lancer entra√Ænement simple PPO
- [ ] V√©rifier convergence
- [ ] Comparer avec r√©sultats rl-games

### üîÑ Phase 3: Migration agents custom (AWD, AMP, HRL)

#### 3.1 Analyser diff√©rences PPO
- [ ] Comparer `rl_games.A2CAgent` vs `rsl_rl.PPO`
- [ ] Identifier fonctionnalit√©s manquantes dans RSL-RL
- [ ] Documenter adaptations n√©cessaires

#### 3.2 Cr√©er CustomPPO pour AWD
- [ ] Cr√©er `awd_isaaclab/learning/awd_ppo.py`
- [ ] H√©riter de `rsl_rl.algorithms.PPO`
- [ ] Ajouter discriminateur (comme dans `awd_agent.py`)
- [ ] Ajouter encodeur pour diversit√©
- [ ] Ajouter replay buffer AMP
- [ ] Impl√©menter compute_disc_reward()
- [ ] Impl√©menter compute_enc_reward()
- [ ] Modifier loss pour inclure disc_loss + enc_loss

**Composants √† porter:**
```python
# De old_awd/learning/awd_agent.py
- _amp_debug()
- _disc_loss()
- _enc_loss()
- _fetch_amp_obs_demo()
- _update_amp_demos()
- compute_disc_reward()
- compute_enc_reward()
```

#### 3.3 Cr√©er CustomPPO pour AMP
- [ ] Cr√©er `awd_isaaclab/learning/amp_ppo.py`
- [ ] H√©riter de `rsl_rl.algorithms.PPO`
- [ ] Ajouter discriminateur uniquement
- [ ] Ajouter replay buffer AMP
- [ ] Impl√©menter compute_disc_reward()

#### 3.4 Cr√©er CustomPPO pour HRL
- [ ] Cr√©er `awd_isaaclab/learning/hrl_ppo.py`
- [ ] Impl√©menter low-level policy
- [ ] Impl√©menter high-level policy
- [ ] G√©rer latent skills

### üîÑ Phase 4: R√©seaux de neurones

#### 4.1 Analyser r√©seaux actuels
- [ ] √âtudier `awd_network_builder.py`
- [ ] √âtudier `amp_network_builder.py`
- [ ] √âtudier `hrl_network_builder.py`

#### 4.2 Cr√©er modules r√©seau RSL-RL
- [ ] Cr√©er `awd_isaaclab/learning/networks/awd_actor_critic.py`
- [ ] Cr√©er discriminateur r√©seau
- [ ] Cr√©er encodeur r√©seau
- [ ] H√©riter de `rsl_rl.modules.ActorCritic`

**Architecture √† reproduire:**
```
Actor:
  - MLP: [1024, 1024, 512] + ReLU
  - Output: actions

Critic:
  - MLP: [1024, 1024, 512] + ReLU
  - Output: value

Discriminateur (AMP/AWD):
  - MLP: [1024, 1024, 512] + ReLU
  - Output: real/fake logit

Encodeur (AWD):
  - MLP: [1024, 512] + ReLU
  - Output: latent encoding
```

### üîÑ Phase 5: Configuration et hyperparam√®tres

#### 5.1 Mapper configs rl-games -> RSL-RL
- [ ] Cr√©er tableau de correspondance des param√®tres
- [ ] Adapter configs PPO

**Mapping initial:**
| rl-games | RSL-RL | Notes |
|----------|--------|-------|
| `horizon_length` | `num_steps_per_env` | Rollout length |
| `minibatch_size` | `num_mini_batches` | Calcul√© diff√©remment |
| `mini_epochs` | `num_learning_epochs` | Epochs par update |
| `learning_rate` | `learning_rate` | Identique |
| `gamma` | `gamma` | Discount factor |
| `tau` | `lam` | GAE lambda |
| `e_clip` | `clip_param` | PPO clip |
| `entropy_coef` | `entropy_coef` | Identique |

#### 5.2 Cr√©er dataclasses configuration
- [ ] `AWDPPOCfg` avec tous les hyperparams AWD
- [ ] `AMPPPOCfg` avec tous les hyperparams AMP
- [ ] `HRLPPOCfg` avec tous les hyperparams HRL

### üîÑ Phase 6: Players (Inf√©rence)

#### 6.1 Cr√©er players RSL-RL
- [ ] Analyser `old_awd/learning/*_players.py`
- [ ] Cr√©er script d'inf√©rence `awd_isaaclab/scripts/play_rsl_rl.py`
- [ ] Charger checkpoint RSL-RL
- [ ] Ex√©cuter politique en mode eval

### üîÑ Phase 7: Utilitaires et logging

#### 7.1 TensorBoard logging
- [ ] Adapter logging pour RSL-RL
- [ ] Logger m√©triques custom (disc_loss, enc_loss, etc.)
- [ ] Logger rewards AMP/AWD

#### 7.2 Checkpointing
- [ ] Configurer sauvegarde checkpoints
- [ ] Impl√©menter best model saving
- [ ] Tester chargement checkpoints

### üîÑ Phase 8: Tests et validation

#### 8.1 Tests unitaires
- [ ] Tester chaque agent s√©par√©ment
- [ ] Tester r√©seaux de neurones
- [ ] Tester compute_reward custom

#### 8.2 Tests d'entra√Ænement
- [ ] DucklingCommand avec PPO simple
- [ ] DucklingCommand avec AWD
- [ ] DucklingHeading avec PPO
- [ ] DucklingAMP avec AMP

#### 8.3 Validation r√©sultats
- [ ] Comparer courbes d'apprentissage rl-games vs RSL-RL
- [ ] V√©rifier convergence
- [ ] Valider performance finale

### üîÑ Phase 9: Documentation

#### 9.1 Mise √† jour docs
- [ ] Mettre √† jour README avec instructions RSL-RL
- [ ] Documenter nouveaux scripts train/play
- [ ] Cr√©er guide de migration pour utilisateurs

#### 9.2 Cleanup
- [ ] Supprimer code rl-games obsol√®te (optionnel)
- [ ] Nettoyer imports
- [ ] V√©rifier d√©pendances requirements.txt

---

## 4. Correspondance des fichiers

### Old (rl-games) ‚Üí New (RSL-RL)

| Ancien fichier | Nouveau fichier | Status |
|----------------|-----------------|--------|
| `awd_isaaclab/scripts/run_isaaclab.py` | `awd_isaaclab/scripts/train_rsl_rl.py` | ‚è≥ √Ä cr√©er |
| `old_awd/learning/common_agent.py` | `rsl_rl.algorithms.PPO` (base) | ‚úÖ Built-in |
| `old_awd/learning/awd_agent.py` | `awd_isaaclab/learning/awd_ppo.py` | ‚è≥ √Ä cr√©er |
| `old_awd/learning/amp_agent.py` | `awd_isaaclab/learning/amp_ppo.py` | ‚è≥ √Ä cr√©er |
| `old_awd/learning/hrl_agent.py` | `awd_isaaclab/learning/hrl_ppo.py` | ‚è≥ √Ä cr√©er |
| `old_awd/learning/*_network_builder.py` | `awd_isaaclab/learning/networks/*.py` | ‚è≥ √Ä cr√©er |
| `old_awd/learning/*_players.py` | `awd_isaaclab/scripts/play_rsl_rl.py` | ‚è≥ √Ä cr√©er |
| `old_awd/data/cfg/*/train/*.yaml` | `awd_isaaclab/configs/train/*_cfg.py` | ‚è≥ √Ä cr√©er |

---

## 5. D√©tails techniques importants

### 5.1 Diff√©rences API cl√©s

#### Environnement:
```python
# rl-games (ancien)
class RLGPUEnv(vecenv.IVecEnv):
    def step(self, action):
        # Retourne 4 valeurs: obs, reward, done, info
        return obs, reward, done, info

# RSL-RL (nouveau)
class RslRlVecEnvWrapper:
    def step(self, actions):
        # Retourne VecEnvStepReturn avec obs, privileged_obs, rewards, dones, infos
        return VecEnvStepReturn(...)
```

#### Observations:
```python
# rl-games: obs simple ou dict {"obs": obs, "states": states}
# RSL-RL: dict {"policy": obs} avec support privileged_obs
```

### 5.2 Gestion du replay buffer AMP

Dans rl-games (ancien):
```python
# old_awd/learning/awd_agent.py
self._amp_obs_demo_buffer  # Buffer des demos
self._amp_replay_buffer     # Replay buffer
```

Dans RSL-RL (nouveau):
- Cr√©er classe `AMPReplayBuffer` custom
- Stocker dans agent custom AWD/AMP
- Utiliser lors du calcul disc_loss

### 5.3 Calcul des rewards

#### AWD (style imitatif):
```python
# Reward total = task_reward_w * task_rew + disc_reward_w * disc_rew + enc_reward_w * enc_rew
total_reward = (
    self.task_reward_w * task_rewards +
    self.disc_reward_w * disc_rewards +
    self.enc_reward_w * enc_rewards
)
```

#### AMP (imitation pure):
```python
# Reward total = disc_reward_scale * disc_rew
total_reward = self.disc_reward_scale * disc_rewards
```

---

## 6. Checklist de validation

### Avant de consid√©rer la migration termin√©e:

- [ ] Tous les agents fonctionnent (PPO, AWD, AMP, HRL)
- [ ] Convergence comparable √† rl-games
- [ ] Performance finale >= rl-games
- [ ] Checkpointing fonctionne
- [ ] Inf√©rence (play) fonctionne
- [ ] Logs TensorBoard corrects
- [ ] Documentation √† jour
- [ ] Tests passent
- [ ] Code nettoy√© et comment√©

---

## 7. Commandes de test

### Entra√Ænement:
```bash
# PPO simple
./run_with_isaaclab.sh awd_isaaclab/scripts/train_rsl_rl.py \
    --task DucklingCommand --robot go_bdx --num_envs 4096

# AWD
./run_with_isaaclab.sh awd_isaaclab/scripts/train_rsl_rl.py \
    --task DucklingCommand --robot go_bdx --algo awd --num_envs 4096

# AMP
./run_with_isaaclab.sh awd_isaaclab/scripts/train_rsl_rl.py \
    --task DucklingAMP --robot go_bdx --algo amp --num_envs 4096
```

### Inf√©rence:
```bash
./run_with_isaaclab.sh awd_isaaclab/scripts/play_rsl_rl.py \
    --task DucklingCommand --robot go_bdx --checkpoint runs/DucklingCommand_go_bdx/model.pt
```

---

## 8. Ressources et r√©f√©rences

### Documentation:
- RSL-RL: https://github.com/leggedrobotics/rsl_rl
- Isaac Lab workflows: `IsaacLab/source/standalone/workflows/rsl_rl/`
- Isaac Lab wrappers: `isaaclab/utils/wrappers/rsl_rl/`

### Exemples Isaac Lab:
- `train.py` - Entra√Ænement de base
- `play.py` - Inf√©rence
- Configuration examples dans envs

### Code de r√©f√©rence (ancien):
- `old_awd/run.py` - Structure principale
- `old_awd/learning/` - Tous les agents custom

---

## 9. Notes de progression

### [DATE] - Phase X
- T√¢ches compl√©t√©es
- Probl√®mes rencontr√©s
- Solutions appliqu√©es

---

**Statut g√©n√©ral: üîÑ EN COURS - Phases 0-2 partiellement compl√©t√©es**

## Statut d√©taill√© par phase

### Phase 0: Pr√©paration ‚úÖ COMPL√âT√â
- [x] Analyse du code actuel rl-games
- [x] Identification des composants √† migrer
- [x] Cr√©ation de ce document de suivi

### Phase 1: Configuration de base ‚úÖ COMPL√âT√â
- [x] RSL-RL install√© (version 3.1.3)
- [x] tensorboard install√©
- [x] Structure de configuration cr√©√©e: `awd_isaaclab/configs/agents/`
- [x] Configuration PPO cr√©√©e: `rsl_rl_ppo_cfg.py`

### Phase 2: Script d'entra√Ænement de base ‚úÖ PARTIELLEMENT COMPL√âT√â
- [x] Script `train_rsl_rl.py` cr√©√©
- [x] Hyperparam√®tres PPO configur√©s (identiques √† rl-games)
- [x] API DirectRLEnv corrig√©e (_apply_action impl√©ment√©)
- [ ] **PROBL√àME**: Environnement bloque au d√©marrage de l'entra√Ænement

**Prochaine √©tape: D√©bugger le blocage de l'environnement pendant l'entra√Ænement**

## Notes de d√©bogage (2025-11-22)

### Probl√®mes identifi√©s:
1. L'environnement se cr√©e correctement
2. RSL-RL OnPolicyRunner d√©marre
3. Le processus se bloque apr√®s les premiers warnings Gymnasium
4. Warnings sur types Tensor vs numpy (normal avec RslRlVecEnvWrapper)

### Solutions possibles √† tester:
1. D√©sactiver le passive_env_checker de Gymnasium
2. V√©rifier la compatibilit√© headless mode
3. Tester avec viewer activ√© pour voir si c'est un probl√®me headless
4. Augmenter le timeout
5. V√©rifier les logs Isaac Sim d√©taill√©s (/tmp/isaaclab_*.log)
